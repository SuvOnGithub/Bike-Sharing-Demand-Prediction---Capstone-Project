{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuvOnGithub/Bike-Sharing-Demand-Prediction---Capstone-Project/blob/main/suvenddu_Bike_Sharing_Demand_Prediction_Capstone_Project_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Cu28WmkU4_3"
      },
      "source": [
        "# <b><u> Project Title : Seoul Bike Sharing Demand Prediction </u></b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kND9iSmPVBRi"
      },
      "source": [
        "## <b> Problem Description </b>\n",
        "\n",
        "### Currently Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concern. The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEB4-jsBVIDw"
      },
      "source": [
        "## <b> Data Description </b>\n",
        "\n",
        "### <b> The dataset contains weather information (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour and date information.</b>\n",
        "\n",
        "\n",
        "### <b>Attribute Information: </b>\n",
        "\n",
        "* ### Date : year-month-day\n",
        "* ### Rented Bike count - Count of bikes rented at each hour\n",
        "* ### Hour - Hour of he day\n",
        "* ### Temperature-Temperature in Celsius\n",
        "* ### Humidity - %\n",
        "* ### Windspeed - m/s\n",
        "* ### Visibility - 10m\n",
        "* ### Dew point temperature - Celsius\n",
        "* ### Solar radiation - MJ/m2\n",
        "* ### Rainfall - mm\n",
        "* ### Snowfall - cm\n",
        "* ### Seasons - Winter, Spring, Summer, Autumn\n",
        "* ### Holiday - Holiday/No holiday\n",
        "* ### Functional Day - NoFunc(Non Functional Hours), Fun(Functional hours)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umB8sWszVTpA"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Eu0zF-9Vlnc"
      },
      "source": [
        "Rental Bike Sharing is the process by which bicycles are procured on several basis- hourly, weekly,membership-wise, etc. This phenomenon has seen its stockrise to considerable levels due to a global effort towards reducing the carbon footprint, leading to climate change,unprecedented natural disasters, ozone layer depletion,and other environmental anomalies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfQovzSmVmiw"
      },
      "source": [
        "In my project, I chose to analyse a dataset pertaining to Rental Bike Demand from South Koreancity of Seoul, comprising of climatic variables like Temperature, Humidity, Rainfall, Snowfall, Dew Point Temperature, and others. For the available raw data,firstly, a through pre-processing was done after which a Here, hourly rental bike count is the regress."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr6Q6furVpl4"
      },
      "source": [
        "\n",
        "Bike sharing systems allow the users to take one way bicycle trips over short distances.Generally these systems are operated via automated kiosks to save manpower and reduce waiting time for the users. Bike Sharing System ensures that pollution is reduced as with use of bicycles there is reduction in use of motor vehicles which leads to reduction in emission of pollutants in the air. This practice of Bike Sharing Systems is common in Western Countries while the same is not seen yet in countries like India.In India most of the bike sharing systems could not achieve their maximum potential as data analysis was not used properly. The advantages of this system is that we can have public bike stations without any human involvement. Even local Chennai Municipal Corporation has invited biddings for a new bicycle sharing system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCiUsuPaVvsd"
      },
      "source": [
        "Generally in bicycle sharing systems it is very important that the administrators should know how many cycles will be needed in each bicycle station, knowing this count enables them the arrange proper number of cycles at the stations and decide whether a particular station needs to have extra number of bicycle stands.So in this research work we study various prediction algorithms i.e. linear regression, decision trees, gradient boosting machines. This research work focuses on which algorithm can work better for the real world problem of bicycle sharing demand prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXIZIaLxVzf_"
      },
      "source": [
        "##**Business Goal:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYKm39Y1V0iD"
      },
      "source": [
        "We are required to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "didGUzTglIdj"
      },
      "outputs": [],
      "source": [
        "#import required packages and libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# import plotly.graph_objects as go\n",
        "# import plotly.express as px\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ctl_NmadWKGZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSO3hy-sXVcY"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qErOv_HzXm5z"
      },
      "outputs": [],
      "source": [
        "#importing file and converting it into dataframe\n",
        "file='/content/drive/MyDrive/alma better/SeoulBikeData.csv'\n",
        "csv=pd.read_csv(file,encoding='ISO-8859-1')\n",
        "df=pd.DataFrame(csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKSq4S5NX0fi"
      },
      "outputs": [],
      "source": [
        "df.head()#getting first 5 rows of dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL8j1WPuX4Uo"
      },
      "outputs": [],
      "source": [
        "df.tail()#getting last 5 rows of dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_knQRZ7SX8RR"
      },
      "outputs": [],
      "source": [
        "df.shape#getting shape of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDLpZDvSYAJN"
      },
      "outputs": [],
      "source": [
        "df.describe(include='all')#getting summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tj4f5L6aa4g"
      },
      "outputs": [],
      "source": [
        "#check information about the data\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVtd6hPvYDzo"
      },
      "outputs": [],
      "source": [
        "#finding null values\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8zS5qLdX0MT"
      },
      "source": [
        "**OBSERVATION:** There is no null values present in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTO5a1yqb45c"
      },
      "outputs": [],
      "source": [
        "#getting summary\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0pHdd1odIb8"
      },
      "outputs": [],
      "source": [
        "#converting the date column into year, day, month, weekday column. \n",
        "df['Date']=pd.to_datetime(df['Date'])\n",
        "df['Year'] = pd.DatetimeIndex(df['Date']).year\n",
        "df['Day'] = pd.DatetimeIndex(df['Date']).day\n",
        "df['Month']= pd.DatetimeIndex(df['Date']).month\n",
        "df['weekday']=pd.DatetimeIndex(df['Date']).weekday"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZtUNNAOdKcT"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRHGRNVhdMMj"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiCXDQ_NddMk"
      },
      "outputs": [],
      "source": [
        "display(df['weekday'].unique())#getting unique values of week"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8cEHPSydig8"
      },
      "source": [
        "Weekday is in range 0-6 so we need to make it in normal 1-7."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6YJYNaudjVk"
      },
      "outputs": [],
      "source": [
        "#previously weekday values was in 0-6 now we are converting it into 1-7 format\n",
        "df['weekday'] = np.array(df['weekday'])+1\n",
        "display(df['weekday'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S3W6Sj_d0zD"
      },
      "source": [
        "##Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgHk3vyGecyE"
      },
      "source": [
        "Exploratory data analysis is an statistical way of understanding the data which is usually done in a visual way.The graphs plotted in explotary data analysis are for better understanding of data to the analyst.\n",
        "\n",
        "Since we have to predict the number of bikes that will be rented, the best way to begin is with the variable to predict, \"Rented Bike Count\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzbjJyjNefaD"
      },
      "source": [
        "##**Demand of rented bikes at different times of years**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_h75c434eiEC"
      },
      "outputs": [],
      "source": [
        "#creating data frame with year, month, day, weekday and rented bike count\n",
        "Rented_bike_per_year = pd.DataFrame(df['Rented Bike Count'].groupby(by=df['Year']).sum()).reset_index().sort_values(\"Year\", ascending=True)\n",
        "Rented_bike_per_month = pd.DataFrame(df['Rented Bike Count'].groupby(by=df['Month']).sum()).reset_index().sort_values(\"Month\", ascending=True)\n",
        "Rented_bike_per_Day= pd.DataFrame(df['Rented Bike Count'].groupby(by=df['Day']).sum()).reset_index().sort_values(\"Day\", ascending=True)\n",
        "Rented_bike_per_Weekday= pd.DataFrame(df['Rented Bike Count'].groupby(by=df['weekday']).sum()).reset_index().sort_values(\"weekday\", ascending=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2f9yQL3hTAi"
      },
      "source": [
        "RENTED BIKE COUNT PER YEAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWrCvHqFhVdi"
      },
      "outputs": [],
      "source": [
        "Rented_bike_per_year\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECnkZ02FiGhh"
      },
      "outputs": [],
      "source": [
        "# Defining the plot size\n",
        "plt.figure(figsize=(8, 8))\n",
        " \n",
        "# Defining the values for x-axis, y-axis\n",
        "# and from which datafarme the values are to be picked\n",
        "plots = sns.barplot(x=Rented_bike_per_year['Year'] , y=Rented_bike_per_year['Rented Bike Count'])\n",
        "# Iterrating over the bars one-by-one\n",
        "for bar in plots.patches:\n",
        "    \n",
        "# Using Matplotlib's annotate function and\n",
        "# passing the coordinates where the annotation shall be done\n",
        "# x-coordinate: bar.get_x() + bar.get_width() / 2\n",
        "# y-coordinate: bar.get_height()\n",
        "# free space to be left to make graph pleasing: (0, 8)\n",
        "# ha and va stand for the horizontal and vertical alignment\n",
        "    plots.annotate(format(bar.get_height(), '.2f'),\n",
        "                (bar.get_x() + bar.get_width() / 2,\n",
        "                    bar.get_height()), ha='center', va='center',\n",
        "                size=15, xytext=(0, 8),\n",
        "                textcoords='offset points')\n",
        " \n",
        "\n",
        " \n",
        "# Setting the label for x-axis\n",
        "plt.xlabel(\"Years\", size=14)\n",
        " \n",
        "# Setting the label for y-axis\n",
        "plt.ylabel(\"Rented Bike Count\", size=14)\n",
        " \n",
        "# Setting the title for the graph\n",
        "plt.title(\"This is an annotated barplot\")\n",
        " \n",
        "# Fianlly showing the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TqxCnHrjtyp"
      },
      "source": [
        "**OBSERVATION:** Here we can see in year 2018 the rented bike count was 5986984 which is greater than 2017. It is because this business was started in 2017 and after one year business got accelerated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coFJTfvIj2uD"
      },
      "source": [
        "RENTED BIKE COUNT PER MONTH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mbfu2z10jtiD"
      },
      "outputs": [],
      "source": [
        "Rented_bike_per_month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QL-4BmHpj-Cx"
      },
      "outputs": [],
      "source": [
        "# Defining the plot size\n",
        "plt.figure(figsize=(18, 8))\n",
        " \n",
        "# Defining the values for x-axis, y-axis\n",
        "# and from which datafarme the values are to be picked\n",
        "plots = sns.barplot(x=Rented_bike_per_month['Month'] , y=Rented_bike_per_month['Rented Bike Count'])\n",
        " \n",
        "# Iterrating over the bars one-by-one\n",
        "for bar in plots.patches:\n",
        "    \n",
        "# Using Matplotlib's annotate function and\n",
        "# passing the coordinates where the annotation shall be done\n",
        "# x-coordinate: bar.get_x() + bar.get_width() / 2\n",
        "# y-coordinate: bar.get_height()\n",
        "# free space to be left to make graph pleasing: (0, 8)\n",
        "# ha and va stand for the horizontal and vertical alignment\n",
        "    plots.annotate(format(bar.get_height(), '.2f'),\n",
        "                (bar.get_x() + bar.get_width() / 2,\n",
        "                    bar.get_height()), ha='center', va='center',\n",
        "                size=15, xytext=(0, 8),\n",
        "                textcoords='offset points')\n",
        " \n",
        "# Setting the label for x-axis\n",
        "plt.xlabel(\"Months\", size=14)\n",
        "\n",
        " \n",
        "# Setting the label for y-axis\n",
        "plt.ylabel(\"Rented Bike Count\", size=14)\n",
        " \n",
        "# Setting the title for the graph\n",
        "plt.title(\"This is an annotated barplot\")\n",
        " \n",
        "# Fianlly showing the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcxdbBmQkEn6"
      },
      "source": [
        "**OBSERVATION:** Here we can see in 6th month or in june the rented bike count is 706728 which is highest and in 2nd month or in feb the count was lowest which is 264112."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19WcNNNrkGsx"
      },
      "source": [
        "RENTED BIKE COUNT PER DAY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjN2rZW4lSJJ"
      },
      "outputs": [],
      "source": [
        "Rented_bike_per_Day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsjaB5U5lahC"
      },
      "outputs": [],
      "source": [
        "# Defining the plot size\n",
        "plt.figure(figsize=(22, 8))\n",
        " \n",
        "# Defining the values for x-axis, y-axis\n",
        "# and from which datafarme the values are to be picked\n",
        "plots = sns.barplot(x=Rented_bike_per_Day['Day'] , y=Rented_bike_per_Day['Rented Bike Count'])\n",
        " \n",
        "# Iterrating over the bars one-by-one\n",
        "for bar in plots.patches:\n",
        "    \n",
        "# Using Matplotlib's annotate function and\n",
        "# passing the coordinates where the annotation shall be done\n",
        "# x-coordinate: bar.get_x() + bar.get_width() / 2\n",
        "# y-coordinate: bar.get_height()\n",
        "# free space to be left to make graph pleasing: (0, 8)\n",
        "# ha and va stand for the horizontal and vertical alignment\n",
        "    plots.annotate(format(bar.get_height(), '.2f'),\n",
        "                (bar.get_x() + bar.get_width() / 2,\n",
        "                    bar.get_height()), ha='center', va='center',\n",
        "                size=15, xytext=(0, 8),\n",
        "                textcoords='offset points')\n",
        " \n",
        "# Setting the label for x-axis\n",
        "plt.xlabel(\"Days\", size=14)\n",
        " \n",
        "# Setting the label for y-axis\n",
        "plt.ylabel(\"Rented Bike Count\", size=14)\n",
        " \n",
        "# Setting the title for the graph\n",
        "plt.title(\"This is an annotated barplot\")\n",
        " \n",
        "# Fianlly showing the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Cexqf7llghi"
      },
      "source": [
        "**OBSERVATION:** Here we can see the rented bike count is highest on 6th day of the month which is 371295 and lowest on 2nd day of the month which is 53694."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfAyBFq_kJK5"
      },
      "source": [
        "RENTED BIKE COUNT PER WEEKDAY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MT_z1FeTkIuR"
      },
      "outputs": [],
      "source": [
        "Rented_bike_per_Weekday"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAZMvZL3M-uR"
      },
      "outputs": [],
      "source": [
        "# Defining the plot size\n",
        "plt.figure(figsize=(15, 8))\n",
        " \n",
        "# Defining the values for x-axis, y-axis\n",
        "# and from which datafarme the values are to be picked\n",
        "plots = sns.barplot(x=Rented_bike_per_Weekday['weekday'] , y=Rented_bike_per_Weekday['Rented Bike Count'])\n",
        " \n",
        "# Iterrating over the bars one-by-one\n",
        "for bar in plots.patches:\n",
        "    \n",
        "# Using Matplotlib's annotate function and\n",
        "# passing the coordinates where the annotation shall be done\n",
        "# x-coordinate: bar.get_x() + bar.get_width() / 2\n",
        "# y-coordinate: bar.get_height()\n",
        "# free space to be left to make graph pleasing: (0, 8)\n",
        "# ha and va stand for the horizontal and vertical alignment\n",
        "    plots.annotate(format(bar.get_height(), '.2f'),\n",
        "                (bar.get_x() + bar.get_width() / 2,\n",
        "                    bar.get_height()), ha='center', va='center',\n",
        "                size=15, xytext=(0, 8),\n",
        "                textcoords='offset points')\n",
        " \n",
        "# Setting the label for x-axis\n",
        "plt.xlabel(\"Weekdays\", size=14)\n",
        " \n",
        "# Setting the label for y-axis\n",
        "plt.ylabel(\"Rented Bike Count\", size=14)\n",
        " \n",
        "# Setting the title for the graph\n",
        "plt.title(\"This is an annotated barplot\")\n",
        " \n",
        "# Fianlly showing the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehlWCxF0NF8U"
      },
      "source": [
        "**OBSERVATION:** Here we can see on 4th day of week the rented bike count is 928267 which is highest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ryu78xaNJRH"
      },
      "source": [
        "RENTED BIKE COUNT WITH RESPECT TO TEMPERATURE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1sXeeWSNfWR"
      },
      "outputs": [],
      "source": [
        " # Plot the graph between the temperature and rented bike counts\n",
        "temp=df.groupby('Temperature(°C)')['Rented Bike Count'].sum()\n",
        "temp.plot.area(color='cyan',figsize=(12, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7jU2YxVNomZ"
      },
      "source": [
        " **Observation:** \n",
        " \n",
        "*  From the graph, we can see that people prefer to take bike ride more often when the temperature is near about 25 degrees Celcius.\n",
        "*   From the above graph, we can easliy conclude that the people gave more preference to bike riding in summers as compare to other seasons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h7v60MSNs2_"
      },
      "source": [
        "MONTHS OF BOTH YEAR 2017 AND 2018 AND RENTED BIKE COUNT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfX7fHqFNv8-"
      },
      "outputs": [],
      "source": [
        " #creating dataframe with months of both year 2017 and 2018 and rented bike count\n",
        "df.groupby(['Year','Month']).agg({'Rented Bike Count':['sum']}).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h35OE6Cbe11S"
      },
      "outputs": [],
      "source": [
        " #plotting graph\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.barplot(x = 'Month', y = 'Rented Bike Count', data =df, hue = 'Year')\n",
        "plt.title(\"Total number of bikes rented in different months\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEi23IDEe8Mh"
      },
      "source": [
        " **OBSERVATION:** \n",
        "1.   There's is a whooping increase in number of bike rents in year 2018.\n",
        "2.    In the last month the demand decreases in 2018 but increases in it seen to be increasing in the end of 2017.\n",
        "3.    It is like this because, in 2017 the demand is taking off and we can see the pattern as it is still inceasing in the beginning months of 2018.\n",
        "4.    There is a decline in the end of the year. This could be repercussions of winter season as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ3WTi-1fC6I"
      },
      "outputs": [],
      "source": [
        "  df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gRGCuIKxLyl"
      },
      "outputs": [],
      "source": [
        " #extracting unique seasons\n",
        "df['Seasons'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kedscuh9xP6Z"
      },
      "outputs": [],
      "source": [
        " #adding temperature and dew point temperature column \n",
        "df['Temperature_and_DP_Temp'] = [df['Temperature(°C)'][i]+df['Dew point temperature(°C)'][i] for i in range(len(df))]\n",
        "df.drop(['Temperature(°C)','Dew point temperature(°C)'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTJZ6zRexVNK"
      },
      "source": [
        "RENTED BIKE COUNT PER SEASON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCvbm7WKxUFa"
      },
      "outputs": [],
      "source": [
        " #seasons and Rented Bike Count\n",
        "df.groupby(['Seasons'])['Rented Bike Count'].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmITYjJQxgRH"
      },
      "outputs": [],
      "source": [
        "#plotting pie \n",
        "df.groupby('Seasons')['Rented Bike Count'].sum().plot.pie(figsize=(15,8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHA_ntFYxixX"
      },
      "outputs": [],
      "source": [
        " #plotting graph\n",
        "plt.figure(figsize=(15,8))\n",
        "sns.barplot(x = 'Seasons', y = 'Rented Bike Count', data = df)\n",
        "plt.title(\"Total number of bikes rented in different Seasons\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkytIWO8xuzi"
      },
      "source": [
        " **OBSERVATION:** Here with pie and bar plot we can say in summer the rented bike count was high as compared to other seasons and lowest in winter season. This is because when temperature decreases amount of snowfall increases due to which people avoid getting out that is the reason in summer rented bike count increases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrZPnlQ0xxT_"
      },
      "source": [
        "RENTED BIKE COUNT ON HOLIDAY AND ON NO HOLIDAY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwDuwhwcx0Fw"
      },
      "outputs": [],
      "source": [
        " #creating dataframe with holiday and rented bike count\n",
        "holiday_bike_count=pd.DataFrame(df.groupby('Holiday')['Rented Bike Count'].sum()).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnZxihrGyDom"
      },
      "outputs": [],
      "source": [
        " holiday_bike_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHjItNOqyIgH"
      },
      "outputs": [],
      "source": [
        " # Defining the plot size\n",
        "plt.figure(figsize=(8, 8))\n",
        " \n",
        "# Defining the values for x-axis, y-axis\n",
        "# and from which datafarme the values are to be picked\n",
        "plots = sns.barplot(x=holiday_bike_count['Holiday'] , y=holiday_bike_count['Rented Bike Count'])\n",
        " \n",
        "# Iterrating over the bars one-by-one\n",
        "for bar in plots.patches:\n",
        "    \n",
        "# Using Matplotlib's annotate function and\n",
        "# passing the coordinates where the annotation shall be done\n",
        "# x-coordinate: bar.get_x() + bar.get_width() / 2\n",
        "# y-coordinate: bar.get_height()\n",
        "# free space to be left to make graph pleasing: (0, 8)\n",
        "# ha and va stand for the horizontal and vertical alignment\n",
        "    plots.annotate(format(bar.get_height(), '.2f'),\n",
        "                (bar.get_x() + bar.get_width() / 2,\n",
        "                    bar.get_height()), ha='center', va='center',\n",
        "                size=15, xytext=(0, 8),\n",
        "                textcoords='offset points')\n",
        " \n",
        "# Setting the label for x-axis\n",
        "plt.xlabel(\"Holiday\", size=14)\n",
        " \n",
        "# Setting the label for y-axis\n",
        "plt.ylabel(\"Rented Bike Count\", size=14)\n",
        " \n",
        "# Setting the title for the graph\n",
        "plt.title(\"This is an annotated barplot\")\n",
        " \n",
        "# Fianlly showing the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fm7XiFFOyNc7"
      },
      "source": [
        "##Or we can do the same with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwwF8-LTyObD"
      },
      "outputs": [],
      "source": [
        " #importing library\n",
        "from plotnine import*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dptmJREyTKP"
      },
      "outputs": [],
      "source": [
        " #plotting graph\n",
        "ggplot(df)+ aes('Functioning Day',fill='Holiday')+geom_bar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX5Usd1LyXUG"
      },
      "source": [
        " **OBSERVATION:** \n",
        "*   Here's an ironic insight, all the holidays are falling on the functioning Days.\n",
        " \n",
        "*   Here we can say on no holiday the rented bike count is much more high than on holiday."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUxE8N9vGSxQ"
      },
      "source": [
        "RENTED BIKE COUNT PER HOUR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tN2hmZ2QGVoL"
      },
      "outputs": [],
      "source": [
        " #plotting graph\n",
        "plt.figure(figsize=(15,8))\n",
        "sns.lineplot(df['Hour'],df['Rented Bike Count'])\n",
        "sns.barplot(df['Hour'],df['Rented Bike Count'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cbo-BWCGr68"
      },
      "source": [
        " **OBSERVATION:**  Here with the graph we can say on 18th hour of the day there is a huge spike in the count of rented bike which is approx. 1600."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMBfTfWfGxXE"
      },
      "outputs": [],
      "source": [
        " import seaborn as sns\n",
        "#plotting graph\n",
        "fig, ax=plt.subplots(figsize=(15,8))\n",
        "sns.histplot(data=df, x=\"Rented Bike Count\", kde= True,ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyV5tPtiHD9B"
      },
      "source": [
        "**OBSERVATION:**\n",
        "\n",
        "\n",
        "  \n",
        " \n",
        "*   The data is positively skewed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXam7R5JHFBC"
      },
      "outputs": [],
      "source": [
        " import seaborn as sns\n",
        "#plotting graph\n",
        "fig, ax=plt.subplots(figsize=(15,8))\n",
        "sns.histplot(data=df, x=np.sqrt(df[\"Rented Bike Count\"]), kde= True,ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qpP8XRSHZYp"
      },
      "source": [
        "   **Observation:**\n",
        "*   After squar root transformation it is looking bit normal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnA2jeDNHejG"
      },
      "source": [
        "FINDING CORRELATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtGTHe8KHhTr"
      },
      "outputs": [],
      "source": [
        " #plotting graph\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.heatmap(df.corr(\"pearson\"),\n",
        "            vmin=-1, vmax=1,\n",
        "            cmap='coolwarm',\n",
        "            annot=True, \n",
        "            square=True);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tvqgHARHnjb"
      },
      "source": [
        " **OBSERVATION:** \n",
        "*   Temperature and Dew point temperature are highly correlated. We can add them to make one single column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEpTfYlYEuvm"
      },
      "source": [
        "##Pre-Processing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NZrodXBE1dy"
      },
      "source": [
        "There is a need of data pre-processing because the data\n",
        "may be incomplete or inconsistent or noisy. There are\n",
        "many ways to deal with un-processed data viz:\n",
        "\n",
        "i)Data Cleaning: By this term we mean to fill the missing\n",
        "values in data, identifying and removing outliers in data,\n",
        "smoothningdata.\n",
        "\n",
        "ii)Data Transformation: In this stage operations like\n",
        "normalization and aggrigation are performed.\n",
        "\n",
        "iii)Data Reduction: In this stage the data set is modified\n",
        "such that the results produced by the model are almost the\n",
        "same but un neccesary values in dataset are removed.\n",
        "\n",
        "iv)Data Integration: In this stage data is merged from\n",
        "different sources if needed , again redundancies are\n",
        "removed too.\n",
        "\n",
        "v)Label Encoding: converting the categorical variables into numerical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP6IonylE5XP"
      },
      "source": [
        "##**Label Encoding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11lgHAY7E8TW"
      },
      "source": [
        "We will create DUMMY variables for 3 categorical variables 'Holiday', 'Functioning Day' and 'Seasons'.\n",
        "\n",
        "Before creating dummy variables, we will have to convert them into 'category' data types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dN1YN5pyT5Dd"
      },
      "outputs": [],
      "source": [
        "#replacing no with 0 and yes with 1 and holiday with 1 no holiday with 0\n",
        "bike_df = df.replace({'No':0,'Yes':1,'Holiday':1,'No Holiday':0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bftrp_kQT7uy"
      },
      "outputs": [],
      "source": [
        "#creating dummy varaible for seasons\n",
        "season_dummy = pd.get_dummies(bike_df['Seasons'])\n",
        "for i in season_dummy.columns:\n",
        "  bike_df[i]= season_dummy[i]\n",
        "bike_df.drop('Seasons',axis='columns',inplace=True)\n",
        "bike_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jk6Sg7reUBHv"
      },
      "outputs": [],
      "source": [
        "# dropping date column as it is not required\n",
        "bike_df.drop('Date', axis = 1 ,inplace= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srlB9_1jUEi6"
      },
      "outputs": [],
      "source": [
        "#replacing month no. with mnth name\n",
        "bike_df['Month'].replace((1,2,3,4,5,6,7,8,9,10,11,12),('jan','feb','mar','apr','may','june','july','aug','sept','oct','nov','dec'),inplace= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjxOIQAwUISZ"
      },
      "outputs": [],
      "source": [
        "#creating dummy varaible for month\n",
        "month_dummy = pd.get_dummies(bike_df['Month'])\n",
        "for i in month_dummy.columns:\n",
        "  bike_df[i]= month_dummy[i]\n",
        "bike_df.drop('Month',axis='columns',inplace=True)\n",
        "bike_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1J9o8jd1ULd8"
      },
      "outputs": [],
      "source": [
        "bike_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPGd0HQSUNBu"
      },
      "outputs": [],
      "source": [
        "bike_df['Holiday'].value_counts()#getting value count of holidays and no holiday"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYNHZvMRUVG1"
      },
      "outputs": [],
      "source": [
        "#creating dummy varaible for Holiday\n",
        "Holiday_dummy = pd.get_dummies(bike_df['Holiday'])\n",
        "for i in Holiday_dummy.columns:\n",
        "  bike_df[i]= Holiday_dummy[i]\n",
        "bike_df.drop('Holiday',axis='columns',inplace=True)\n",
        "bike_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewC0qryWUbr7"
      },
      "outputs": [],
      "source": [
        "bike_df['Functioning Day'].value_counts()#getting value count of functioning day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hti618awUd0T"
      },
      "outputs": [],
      "source": [
        "#creating dummy varaible for Holiday\n",
        "Functioning_Day_dummy = pd.get_dummies(bike_df['Functioning Day'])\n",
        "for i in Functioning_Day_dummy.columns:\n",
        "  bike_df[i]= Functioning_Day_dummy[i]\n",
        "bike_df.drop('Functioning Day',axis='columns',inplace=True)\n",
        "bike_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T88myQw8Uimw"
      },
      "outputs": [],
      "source": [
        "bike_df.shape# final shape of dataframe after preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlZz2P5dUsu6"
      },
      "source": [
        "##Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MotBbwfPUqvp"
      },
      "outputs": [],
      "source": [
        " #spliting independent and dependent variable\n",
        "x = bike_df.drop('Rented Bike Count',axis=1)\n",
        "y = bike_df['Rented Bike Count']\n",
        "x.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V7cXo_OXIgy"
      },
      "source": [
        "##Train Test split model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7z_Zen4XOiJ"
      },
      "source": [
        " Splitting the data to Train and Test: - We will now split the data into TRAIN and TEST (80:20 ratio)We will use train_test_split method from sklearn package for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKWvhHE7XJ6S"
      },
      "outputs": [],
      "source": [
        " # spliting the model into test and train\n",
        "from sklearn.model_selection import train_test_split \n",
        "x_train, x_test, y_train, y_test = train_test_split( x,y , test_size = 0.2, random_state = 0)\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyPqPzmper8h"
      },
      "source": [
        " **OBSERVATION:** \n",
        "*   We have only 7008 rows of data in our training dataset. We can compromise with our gredient descent to take little longer to reach the global minima. This is why we aren't scaling this particular dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3wDeVN3e_3t"
      },
      "outputs": [],
      "source": [
        " \n",
        "# Import library for VIF\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        " \n",
        "def calc_vif(X):\n",
        " \n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        " \n",
        "    return(vif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKNwDHPJe0en"
      },
      "outputs": [],
      "source": [
        " #calculating vif for all the variables\n",
        "calc_vif(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1IvQfL_fL4s"
      },
      "source": [
        " **OBSERVATION:** \n",
        "We can see here that the 'Temperature_and_DP_Temp   ' have a high VIF value, meaning it can be predicted by other independent variables in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RgBO1GafWal"
      },
      "source": [
        "##Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ucEuyiFfZSh"
      },
      "source": [
        " A machine learning model is built by learning and generalizing from training data, then applying that acquired knowledge to new data it has never seen before to make predictions and fulfill its purpose. Lack of data will prevent you from building the model, and access to data isn't enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rop4DVGmfdFx"
      },
      "source": [
        "##Linear Regression Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An4whXySfgRK"
      },
      "source": [
        "  linear regression is a linear approach to modelling the relationship \n",
        "\n",
        "---\n",
        "\n",
        "between a scalar response and one or more explanatory variables (also known as dependent and independent variables). The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J90cQGlEfhP3"
      },
      "outputs": [],
      "source": [
        " #importing libraries\n",
        "from sklearn.linear_model import LinearRegression\n",
        " \n",
        "import sklearn.metrics as met\n",
        "from sklearn.metrics import mean_squared_error,r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLgfk7qhfqPl"
      },
      "outputs": [],
      "source": [
        " #calling and fitting linear regression\n",
        "linear_reg = LinearRegression()\n",
        "linear_reg.fit(x_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfnCD_KCfulD"
      },
      "outputs": [],
      "source": [
        " # Finding the Evaluation Metrics\n",
        "print (\"training score: \",linear_reg.score(x_train,y_train)) \n",
        "MSE  = mean_squared_error(y_test,linear_reg.predict(x_test))\n",
        "print(\"MSE :\" , MSE)\n",
        " \n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        " \n",
        " \n",
        "r2 = r2_score(y_test,linear_reg.predict(x_test))\n",
        "print(\"R2 :\" ,r2)\n",
        "print(\"Adjusted R2 : \",1-(1-r2_score(y_test,linear_reg.predict(x_test) ))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG3NS7mvQ3k3"
      },
      "source": [
        " **OBSERVATION:** \n",
        "*   We tried adding possible columns to make the model a bit more complex but for Linear Regression model it is still too general\n",
        " \n",
        "*   We have to make our model more complex for better discretion or move to tree and ensembling algorithm for better results.\n",
        " \n",
        "*   After trying combinations of features with linear regression the model underfitted. It seemed obvious because data is spread too much. It didn't seem practical to fit a line.\n",
        " \n",
        "*  Our train score came out to be 0.56 and test score came out to be 0.55."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDo9J-CKQ8Sm"
      },
      "source": [
        "##Ridge and Lasso Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLdBPmNEQ_pQ"
      },
      "source": [
        "  Ridge and Lasso regression are some of the simple techniques to reduce model complexity and prevent over-fitting which may result from simple linear regression. Ridge Regression : In ridge regression, the cost function is altered by adding a penalty equivalent to square of the magnitude of the coefficients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHyCSXX3RG_u"
      },
      "outputs": [],
      "source": [
        " #importing library\n",
        "from sklearn.linear_model import Ridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4iAwm-ZRJe2"
      },
      "outputs": [],
      "source": [
        " rr = Ridge(alpha=0.01) \n",
        "# higher the alpha value, more restriction on the coefficients; low alpha > more generalization,\n",
        "# in this case linear and ridge regression resembles\n",
        "rr.fit(x_train, y_train)\n",
        "rr100 = Ridge(alpha=100) #  comparison with alpha value\n",
        "rr100.fit(x_train, y_train)\n",
        " \n",
        "Ridge_train_score = rr.score(x_train,y_train)\n",
        "Ridge_test_score = rr.score(x_test, y_test)\n",
        "Ridge_train_score100 = rr100.score(x_train,y_train)\n",
        "Ridge_test_score100 = rr100.score(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lUaNMDGRZwg"
      },
      "outputs": [],
      "source": [
        "print (\"training score for alpha=0.01:\",Ridge_train_score) \n",
        "print (\"test score for alpha =0.01: \",Ridge_test_score)\n",
        "print (\"training score for alpha=100:\",Ridge_train_score100) \n",
        "print (\"test score for alpha =100: \", Ridge_test_score100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12DdPq5aRexP"
      },
      "outputs": [],
      "source": [
        "# Finding the Evaluation Metrics\n",
        "MSE  = mean_squared_error(y_test,rr.predict(x_test))\n",
        "print(\"MSE for alpha =0.01 :\" , MSE)\n",
        " \n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE for alpha =0.01 :\" ,RMSE)\n",
        " \n",
        " \n",
        "r2 = r2_score(y_test,rr.predict(x_test))\n",
        "print(\"R2 for alpha =0.01:\" ,r2)\n",
        "print(\"Adjusted R2 for alpha =0.01 : \",1-(1-r2_score(y_test,rr.predict(x_test) ))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jDB9m9iRlFZ"
      },
      "source": [
        " **OBSERVATION:**  \n",
        "*  Here with ridge the train score for alpha=0.01 came to be 0.56 and the test score for alpha=0.01 came to be 0.55.\n",
        " \n",
        "*  With ridge the train score for alpha=100 came to be 0.56 and the test score for alpha=100 came to be 0.55.\n",
        " \n",
        "*  For both alpha 0.01 and 100 the train and test value came to be 0.56 and 0.55 respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E201rpcJRmlg"
      },
      "outputs": [],
      "source": [
        " #importing library\n",
        "from sklearn.linear_model import Lasso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JSyTLgIVpVg"
      },
      "outputs": [],
      "source": [
        " #using lasso\n",
        "lasso = Lasso()\n",
        "lasso.fit(x_train,y_train)\n",
        "train_score=lasso.score(x_train,y_train)\n",
        "test_score=lasso.score(x_test,y_test)\n",
        "coeff_used = np.sum(lasso.coef_!=0)\n",
        "print( \"training score:\", train_score) \n",
        "print (\"test score: \", test_score)\n",
        "print (\"number of features used: \", coeff_used)\n",
        "#taking alpha=0.01\n",
        "lasso001 = Lasso(alpha=0.01, max_iter=10e5)\n",
        "lasso001.fit(x_train,y_train)\n",
        "train_score001=lasso001.score(x_train,y_train)\n",
        "test_score001=lasso001.score(x_test,y_test)\n",
        "coeff_used001 = np.sum(lasso001.coef_!=0)\n",
        "print (\"training score for alpha=0.01:\", train_score001) \n",
        "print (\"test score for alpha =0.01: \", test_score001)\n",
        "print (\"number of features used: for alpha =0.01:\", coeff_used001)\n",
        "#taking alpha=0.0001\n",
        "lasso00001 = Lasso(alpha=0.0001, max_iter=10e5)\n",
        "lasso00001.fit(x_train,y_train)\n",
        "train_score00001=lasso00001.score(x_train,y_train)\n",
        "test_score00001=lasso00001.score(x_test,y_test)\n",
        "coeff_used00001 = np.sum(lasso00001.coef_!=0)\n",
        "print (\"training score for alpha=0.0001:\", train_score00001) \n",
        "print (\"test score for alpha =0.0001: \", test_score00001)\n",
        "print (\"number of features used: for alpha =0.0001:\", coeff_used00001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w78FqOa1Vz9c"
      },
      "outputs": [],
      "source": [
        " # Finding the Evaluation Metrics\n",
        "MSE  = mean_squared_error(y_test,lasso001.predict(x_test))\n",
        "print(\"MSE for alpha=0.01 :\" , MSE)\n",
        " \n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE for alpha-0.01 :\" ,RMSE)\n",
        " \n",
        "print (\"training score: \",lasso001.score(x_train,y_train)) \n",
        " \n",
        "r2 = r2_score(y_test,lasso001.predict(x_test))\n",
        "print(\"R2 for lasso:\" ,r2)\n",
        "print(\"Adjusted R2 for alpha=0.01: \",1-(1-r2_score(y_test,lasso001.predict(x_test) ))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPNSgYduV60n"
      },
      "outputs": [],
      "source": [
        " # Finding the Evaluation Metrics\n",
        "print (\"training score: \",lasso00001.score(x_train,y_train)) \n",
        "MSE  = mean_squared_error(y_test,lasso00001.predict(x_test))\n",
        "print(\"MSE for alpha=0.0001 :\" , MSE)\n",
        " \n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE for alpha-0.0001 :\" ,RMSE)\n",
        " \n",
        " \n",
        "r2 = r2_score(y_test,lasso00001.predict(x_test))\n",
        "print(\"R2 for lasso:\" ,r2)\n",
        "print(\"Adjusted R2 for alpha=0.0001: \",1-(1-r2_score(y_test,lasso00001.predict(x_test) ))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApbZyOevWEFO"
      },
      "source": [
        " **OBSERVATION:**  \n",
        "*  Here with lasso training score came to be 0.56 and test score came to be 0.55.\n",
        " \n",
        "*  Number of features used is 26.\n",
        " \n",
        "*  Training score for alpha=0.01 came to be 0.56 and\n",
        "test score for alpha =0.01 came to be 0.55.\n",
        " \n",
        "*  Number of features used: for alpha =0.01 is 28.\n",
        " \n",
        "*  Training score for alpha=0.0001 came out to be 0.56 and test score for alpha =0.0001 came to be 0.55.\n",
        " \n",
        "*  Number of features used: for alpha =0.0001 is 28."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozffBAJ6ireZ"
      },
      "source": [
        "##**Decision tree regressor model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8mOxp7eiuoY"
      },
      "source": [
        "Decision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjMMDJxRixCQ"
      },
      "outputs": [],
      "source": [
        "#importing library\n",
        "from sklearn.tree import DecisionTreeRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6hmQqSqi2M5"
      },
      "outputs": [],
      "source": [
        "#calling decision tree regressor\n",
        "reg_decision_model=DecisionTreeRegressor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiaeJWR4i8yc"
      },
      "outputs": [],
      "source": [
        "# fit independent varaibles to the dependent variables\n",
        "reg_decision_model.fit(x_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SG-kv1XqjEFT"
      },
      "outputs": [],
      "source": [
        "# Finding the Evaluation Metrics\n",
        "print (\"training score: \",reg_decision_model.score(x_train,y_train)) \n",
        "MSE  = mean_squared_error(y_test,reg_decision_model.predict(x_test))\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "\n",
        "r2 = r2_score(y_test,reg_decision_model.predict(x_test))\n",
        "print(\"R2 :\" ,r2)\n",
        "print(\"Adjusted R2 : \",1-(1-r2_score(y_test,reg_decision_model.predict(x_test) ))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f1KTFIuaTvk"
      },
      "source": [
        "##**Doing hyperparameter tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ntyy-L7aWo2"
      },
      "source": [
        "Hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cv89pZMmaTeh"
      },
      "outputs": [],
      "source": [
        "# calculating different regression metrics\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import decomposition, datasets\n",
        "from sklearn import tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMFsHIkVbWGv"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "#taking min leaf sample from 1 to 50\n",
        "for min_sam_leaf in range(1,50):\n",
        "  DT_reg = DecisionTreeRegressor(criterion='mse',min_samples_leaf=min_sam_leaf)\n",
        "  DT_reg.fit(x_train,y_train)\n",
        "  print(f\"\\nR-sqared on train dataset when min leaf {min_sam_leaf} : {met.r2_score(y_train,DT_reg.predict(x_train))}\")\n",
        "  print(f\"R-sqared on test dataset when min leaf {min_sam_leaf}: {met.r2_score(y_test,DT_reg.predict(x_test))}\")\n",
        "  print(f\"Mean absolute error on test dataset when min leaf {min_sam_leaf}: {met.mean_absolute_error(y_test,DT_reg.predict(x_test))}\")\n",
        "  print(f\"Mean squared error on test dataset when min leaf {min_sam_leaf}: {met.mean_squared_error(y_test,DT_reg.predict(x_test))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZz-uYgkbbam"
      },
      "source": [
        "**OBSERVATION:**\n",
        "\n",
        "*   As expected Decision tree has overfitted the data.\n",
        "*   But it is doing way better than linear regression on test data as well. \n",
        "*   At minimum Sample leaf 5 the model giving highest r-squared score and least errors on test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evH6ftdZbbGP"
      },
      "outputs": [],
      "source": [
        "#calling decision tree regresor\n",
        "DT_reg = DecisionTreeRegressor(criterion='mse',min_samples_leaf=6)\n",
        "DT_reg.fit(x_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLtRNPkhicT8"
      },
      "outputs": [],
      "source": [
        "# Finding the Evaluation Metrics\n",
        "print (\"training score: \",DT_reg.score(x_train,y_train)) \n",
        "MSE  = mean_squared_error(y_test,DT_reg.predict(x_test))\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "\n",
        "r2 = r2_score(y_test,DT_reg.predict(x_test))\n",
        "print(\"R2 :\" ,r2)\n",
        "print(\"Adjusted R2 : \",1-(1-r2_score(y_test,DT_reg.predict(x_test) ))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkT5rcwqifuU"
      },
      "source": [
        "**OBSERVATION:** With Decision tree we reached at the model r squared value of 0.84. We only fitted with minimum number of leaf hyperparameter. With default paremeters it overfitted and reached r-squared at 1 with train dataset but 0.84 with test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXKMd_S7iif6"
      },
      "source": [
        "##**Feature Importance: Decision Tree**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yc76fumhik90"
      },
      "outputs": [],
      "source": [
        "#creating dataframe\n",
        "features = pd.DataFrame(list(zip(DT_reg.feature_importances_,x.columns)),columns=['Score','Features'])\n",
        "features=features.sort_values('Score',ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnJWkjUOio4C"
      },
      "outputs": [],
      "source": [
        "#plotting graph\n",
        "plt.figure(figsize=(15,7))\n",
        "sns.barplot(x=features['Score'],y=features['Features'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoaPPL1Bi9L9"
      },
      "source": [
        "**OBSERVATION:** \n",
        "HOUR and TEMPERATURE_AND_DP_TEMPERATURE column are the main columns helping in prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7uKgLuo_p0J"
      },
      "source": [
        "##**Random Forest Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbSXWycK_vN5"
      },
      "source": [
        "\n",
        "The term “Random Forest Classifier” refers to the classification algorithm made up of several decision trees. The algorithm uses randomness to build each individual tree to promote uncorrelated forests, which then uses the forest's predictive powers to make accurate decisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSkWyGZh_yoD"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# Create the parameter grid based on the results of random search \n",
        "param_grid = {\n",
        "    'bootstrap': [True],\n",
        "    'max_depth': [90, 100, 110],\n",
        "    'max_features': [2, 3],\n",
        "    'min_samples_leaf': [50,60,60],\n",
        "    'min_samples_split': [50,100,150],\n",
        "    'n_estimators': [200, 300, 1000]\n",
        "}\n",
        "# Create a based model\n",
        "rf = RandomForestRegressor()\n",
        "# Instantiate the grid search model\n",
        "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
        "                          cv = 3, n_jobs = -1, verbose = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGd1NZj__2eV",
        "outputId": "8f2bc58f-909e-4def-8232-d95a682159a4"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n"
          ]
        }
      ],
      "source": [
        "# Fit the grid search to the data\n",
        "grid_search.fit(x_train,y_train)\n",
        "grid_search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestRegressor(bootstrap= True,\n",
        " max_depth= 110,\n",
        " max_features= 3,\n",
        " min_samples_leaf= 50,\n",
        " min_samples_split= 100,\n",
        " n_estimators= 300 )\n",
        "rf.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "XTbN44G-B0RG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the Evaluation Metrics\n",
        "print (\"training score: \",rf.score(x_train,y_train))\n",
        "MSE  = mean_squared_error(y_test,rf.predict(x_test))\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "\n",
        "r2 = r2_score(y_test,rf.predict(x_test))\n",
        "print(\"R2 :\" ,r2)\n",
        "print(\"Adjusted R2 : \",1-(1-r2_score(y_test,rf."
      ],
      "metadata": {
        "id": "BU2CMmapChkC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyNTJ3oZ4QCocIE5TgT9v3PC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}